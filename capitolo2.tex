\chapter{The Choice}
\label{cha:2}
In this chapter we explain the motivations that determined  MongoDB as choice for the evaluation and consequently the commission among other NoSQL possibilities.
Following there is a deep description of many Mongo core features mostly taken from the official documentation, that could be a good introduction for interested users.

\section{MongoDB}
\label{sec:1}
In evaluating which NoSQL technology could be the best for our company we aimed for a combination between the best performance,  ease of use and understandability of the product as we had no real expert in this field.
This is probably the main reason that made us put Cassandra as secondary makeshift in the evaluation.
Thanks to his features Mongo was the first choice for evaluation: its JSON-like format for data called BSON \footnote{See section 2.2.1} and the simple configuration of its nodes made him the perfect candidate.
Many built-in functions of Mongo automatize the setup of a server and the creation of collections, for example if you try to \textit{insert} a document inside an undefined collection \textit{foo}, Mongo will automatically create this new collection called \textit{foo}.
If you connect to a new database in a Mongo instance and start inserting data, it will automatically create the schema, the collections and give a unique\textit{\_id} to each one of them  with no need of manual interaction from the developer.
MongoDB derives its name from \textit{“humongous”} which means enormous and it fits the idea of application that it was designed to support.
Its data records (or rows) are called \textit{documents} and are stored in tables called \textit{collections}. So when you insert something into a Mongo database you are adding document X to collection Y.
A collection does not require the same schema for all its documents, some of them can have more or fewer fields than others, but in case of need it is possible to enforce document validation rules in order to accept only documents with the desired schema.

\section{Key features of Mongo}
\label{sec:2}
In this section we introduce all the main features offered by Mongo by default, with a broad overview on them. 
Then we analyze more deeply how Mongo implements those features giving some examples of their usage.

\subsection{BSON data object}
As explained before Mongo uses the BSON \footnote{Binary JavaScript Object Notation - http://bsonspec.org/} documents that are a binary representation for JSON documents, but with more data types.
The\textit{\_id} field is reserved to be used as a primary key and its value must be unique in the collection and of any type other than array.
There are other restrictions on field names: field names cannot start with the dollar “\$” character and not even with the dot “.” character because Mongo uses \textit{dot notation} to access elements of an array or to access the fields of embedded documents.
There is a maximum size for BSON documents of 16 Megabytes. This is to ensure that a single document does not use an excessive amount of RAM, since Mongo uses mostly RAM during its execution, or bandwidth while sending data.
There is of course the possibility to store bigger documents with \textit{GridFS API} \footnote{https://www.compose.com/articles/gridfs-and-mongodb-pros-and-cons} provided by Mongo developer team, but it the case of our evaluation it was not needed as we used a relatively small schema for our default document “Fattura”, shown in the previous example, that is a simplification of the billing documents used in our customer’s software.
Here an example of how a BSON Mongo document looks like, based on the model used in our tests:
\begin{lstlisting}
{
        "_id" : ObjectId("588cc30072dd84338cbdec77"),
        "_class" : "it.tai.domain.Fattura",
        "rIndex" : NumberLong(2264826),
        "firstName" : "Michele",
        "lastName" : "Romani",
        "company" : "Tai Software Solutions",
        "taxCode" : "01020304569",
        "vatCode" : "RMNMHL93R28A470U",
        "address" : "Via Monviso 16",
        "municipality" : "Asola",
        "province" : "MN",
        "phone" : "+39 333 3117688",
        "zipCode" : "46041",
        "birthday" : "28-10-1993",
        "username" : "mromani",
        "password" : "ypaLLdNYSOKvaBQNreWyUvGp",
        "email" : "mromani@tai.it"
}
\end{lstlisting}

\subsection{Rich Query Language and CRUD operations}
Mongo provides its own query language that, like the majority of NoSQL databases, is not based on SQL. All its CRUD operations (Create, Read, Update, Delete) are \textit{atomic} on the level of a single document and target a single collection.
For Create operations Mongo uses the following methods:
\begin{itemize}
	\item \textit{Db.collection.insert( )}
	\item \textit{Db.collection.insertOne( )}
	\item \textit{Db.collection.insertMany( )}
\end{itemize}
And their names easily explain their function.
For Read operations Mongo uses the method \textit{db.collection.find( )} in which is possible to specify query filters or criteria using defined operators such as \textit{\$aggregation, \$min, \$max, \$gt, \$lt} and many others that is possible to find in Mongo official documentation.
For Update operations Mongo can identify which documents to update using same syntax as read operations. Those methods then perform the update:
\begin{itemize}
	\item \textit{Db.collection.update( )}
	\item \textit{Db.collection.updateOne( )}
	\item \textit{Db.collection.updateMany( )}
	\item \textit{Db.collection.replaceOne( )}
\end{itemize}
Like Create operations, those methods explain themselves with their name. The upsert operation is performed by specifying its parameter as true.
At last, Delete operations uses the same criteria as Read and Update operations with the following methods:
\begin{itemize}
	\item \textit{Db.collection.remove( )}
	\item \textit{Db.collection.deleteOne( )}
	\item \textit{Db.collection.deleteMany( )}
\end{itemize}

It is very simple to create query data using those methods as they only need some parameters to work. The more basic usage just needs the \textit{\_id} of the target document as only parameter to work.
This is an example of a query that uses an operator to perform a simple research:
\begin{lstlisting}
/* Query on a collection named 'school' to select students between letter M and Z */
db.school.find({
	students: {
		$in: [ "M", "Z"]
	}
});

/* Translated in SQL language */

SELECT * FROM school WHERE students in ("M", "Z");
\end{lstlisting}

Each parameter in a Mongo method is wrapped between {} braces and more parameters can be nested with inner {} braces. 
This is basically how Mongo performs CRUD operations on data. For more examples on how querying embedded documents or arrays it is possible to consult Mongo documentation for further examples.

\subsection {Availability and scalability}
In Mongo, it is possible to obtain high availability thanks to \textit{Replica Set} \footnote{See section 2.6}, a replication facility that provides \textit{automatic failover} and \textit{data redundancy}. In substance, it is a set of Mongo servers (or nodes) that store the same data set, increasing data availability.
For horizontal scalability instead, Mongo’s core functionality is \textit{Sharding} \footnote{See section 2.7}, a facility that distributes data across a cluster of machine using a \textit{Shard Key} to balance data. In the latest versions, it is even possible to create zones of data that use the \textit{Shard Key} to direct Mongo operations, covered by a particular zone, only to the shards inside that zone.

\section{Indexes}
\label{sec:3}
Indexes are a special data structure used to store a specific field or set, ordered by its value and they are fundamental for Mongo to perform at its best. This allow supporting efficient equality matches and range-based query operations and they can be easily used to sort results with low computational cost.
Every collection has an unique default index \textit{\_id}, created during the creation of the collection and, if not specified in other ways, calculated on the timestamp of the operative system. It is the primary key of the collection and it prevents clients from inserting duplicates, consequently it cannot be dropped. 
In sharded clusters \textit{\_id} is usually used as default \textit{Shard Key} \footnote{See section 2.7.2}, if another field is specified then it must be enforced to be unique.
Indexes typologies are:
\begin{itemize}
	\item \textit{Single Field Index} : classic index on a single field, it can be traversed in both directions for sorting.
	\item \textit{Compound Index} : index on multiple fields, during creation the sorting order must be specified for each field, having 1 for ascending order and  -1 for descending, then they are sequentially applied.
	\item \textit{Multikey Index} : when a compound index holds an array value then it becomes a multikey index having a different key on each element of the array for many combinations with other fields within the index. It is not possible to have more than an array field in an index of this type.
	\item \textit{Geospatial Index} : index that is used for geospatial coordinate data, they can be 2d indexes for planar geography or 2dsphere for spherical geometry.
	\item \textit{Text Index} : an index that supports searching for string content, but it can only store ‘root’ words, for example it cannot store prepositions like “the”, “a”, “or” etc.
	\item \textit{Hashed Index} : this index is used to support has based sharding so it indexes the hash of the value of a field. It can be used only for equality matches and cannot support range-based queries.
\end{itemize}
\textit{Hashed Indexes} are very important for Mongo scalability on multiple nodes 
and they have been used in the evaluation to support all tests with a multi node database. They use a hashing function that collapses embedded documents and then computes the hash for the entire value.
Since Mongo automatically computes the hashes when resolving queries, user applications do not need to compute them obtaining higher performance

\section{Storage Engines}
\label{sec:4}
A storage engine is the component of a database management system responsible of data storaging on disk or in memory. Mongo supports 3 storage engines with different performance depending on specific workloads:
\begin{itemize}
	\item \textit{Wired Tiger} - It is now the default storage engine and provides a document-level concurrency model, also called \textit{checkpointing} and a data compression function that minimizes storage use at the cost of additional CPU.
\textit{Checkpoints} are snapshot of the data saved automatically by Mongo every 60 seconds or after 2 gigabytes of journal data. Thanks to \textit{Wired Tiger}, Mongo can recover data from last checkpoint event without \textit{journaling} data, but wil lose of course any data written after last checkpoint.
\textit{Journal} is a write-ahead transaction log that persists all data modifications between checkpoints and in \textit{Wired Tiger} is active by default, allowing complete data recovery togheter with \textit{checkpoints}.
By default, Wired Tiger internal cache uses 50\% of available RAM - 1 GB or 256 MB.
	\item \textit{MMAPv1} - It is  the original storage engine of Mongo and it is still a good choice for worlkoads with high volume of operations (inserts, reads, updates). It is based on memory mapped files and, like \textit{Wired Tiger}, it uses \textit{Journal} to ensure that every modification  to Mongo data sets are durably written on disk.
With\textit{MMAPv1} Mongo uses the power of 2 sizes allocation so each file has a size that is a power of 2 (32, 64, 128...). The advantages of this strategy are in reducing fragmentation thanks to efficient reuse of freed records and reducing moves thanks to the added padding space given to documents allowing them to grow without requiring a move.
	\item \textit{In-Memory Storage Engine} - It is an enterprise evolution that retains data in-memory for more predictable data latencies that uses document-level concurrency control for write operations. Multiple clients can modify different documents of a collection at the same time as results of this implementation
\end{itemize}

\section{High Availability}
\label{sec:5}

\subsection{Replica Set and Server Selection Algorithm}
\textit{Replica Set} is a group of \textit{mongod} \footnote{https://docs.mongodb.com/manual/reference/program/mongod}instances that maintain a replicated data set. The purpose is to provide redundancy an high availability of data as base for a production deployment. Copies of data are spread on different database servers and is completely fault tolerant against the loss of one or more nodes depending on the total number of replicated servers \footnote{At least one server needs to be active.}.
Clients can send read operations to multiple servers allowing increased read capacity. Mongo provides two operators \textit{w} that represents the setting to confirm a write operation and \textit{j} that represents the write operation on \textit{Journal} \footnote{https://docs.mongodb.com/manual/core/journaling}.
In Mongo by default w:true and j:false, but in a Replica Set  usually journaling should be active and there can be a primary node with w:majority that means that the write concern is sent to all the members of the set.
If a primary node fails the \textit{Selection Algorithm} starts the election of  a new primary between the other members, using an \textit{Arbiter} \footnote{A node with no data set and no dedicated hardware} in case they are even, and when the old primary recovers it will become a secondary.

\subsection{Automatic failover and data redundancy}
After 10 seconds if a primary node does not communicate with other members, they start the election of a new primary. This operation is called \textit{Automatic Failover} and usually takes less than 30 seconds to declare "inaccessible" a primary node and other 10-30 seconds to complete the election.
After this operation if the old primary recovers as secondary it may have outdated data, so it sends a requests to other members to receive an eventual update of its data set.
It may happen that before the Automatic Failover starts the client is still trying to send data to the open connection of the "dead" primary node, in this case all rejeceted documents are saved in a special snapshot that needs to be manually restored into the database.

\section{Horizontal Scalability}
\label{sec:6}

\subsection{Nodes and Shard Keys}
There two ways for addressing system growth: \textit{Vertical Scaling} and \textit{Horizontal Scaling}. Mongo uses \textit{Sharding} to distribute very larg data sets among nodes horizontally distributed, with a sensible gain in performance.
Horizontal Scalability is in fact more efficient than Vertical because it does not require great performance on single machines but only speed and capacity overall the entire system. The trade-off with these advantages is an increased complexity in infrastructure and maintenance of the system,
\textit{Shards} are subsets of sharded data and each shard can be deployed as a replica set with a \textit{mongos} instance, a query router that provides an interface between clients and the sharded cluster.
Configuration settings and metadata of the cluster must be stored in a special server called \textit{Config Server} \footnote{https://docs.mongodb.com/manual/core/sharded-cluster-config-servers}.
When sharding, the user must provide a \textit{Shard Key}\footnote{In case of a not-empty colletion there must be an index that starts with the Shard Key} to Mongo that is a field of the collection that is used to partion data into \textit{chuncks}. Choosing a good shard key may heavily affect performance (either in a positive or negative way) since it will be based on the content of the key. For example an increasing value in a Shard Key will produce unbalanced chunks, requiring the balancer to acheive an even balance of the chunks in a secondary moment across all shards and slowing the entire database.

\subsection{Hashed Sharding, Ranged Sharding, Zones}
There are two ways to shard in Mongo. With \textit{Hashed Sharding} it computes the hash of the shard key field's value and then each chunk is assigned to a range based on the hash of the Shard Key. This kind of distribution improve data distribution, especially if the Shard Key of the data set changes monotonically.
In case of ranged-based queries on the Shard Key it is better to implement a \textit{Ranged Sharding} that divides data into ranges based on the normal Shard Key field's value. Consequently, a range of those keys whose values are "close" will probably reside on the same chunk, allowing targeted operations. \textit{Mongos} will route these operations only to the shards that contain the required data the performance will increase.
Also in this case a bad choice of the shard key may lead to uneven distribution of data, or even bottlenecks.
It is possibile to create \textit{Zones} of sharded data based on the Shard Key to improve the locality of data. Each Zone can be associated with one or more shards of the cluster and each shard can associate with any number of non-conflicting Zones.
If the cluster is balanced, Mongo will move chunks covered by a Zone only to the shards associated with the same Zone. As it is not possible to change the Shard Key after sharding the colletction, it is good practice to consider the possibility of zone sharding before sharding. In case the Shard Key is compund, the range of the zone must include its prefix.

%\section{Some use cases}
%\label{sec:7}




